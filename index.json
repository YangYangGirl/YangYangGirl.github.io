[{"authors":["admin"],"categories":null,"content":"Yang Yang is a second-year Computer Science Ph.D. student at the Australian National University, supervised by Prof. Liang Zheng. She obtained her B.E. degree at the School of Electronic Information and Communications, Huazhong University of Science and Technology. Her research interests lie within deep learning for Computer Vision, particularly in large vision models, model generalization and data-centric problems.\nActively looking for internships!\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://yangyanggirl.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Yang Yang is a second-year Computer Science Ph.D. student at the Australian National University, supervised by Prof. Liang Zheng. She obtained her B.E. degree at the School of Electronic Information and Communications, Huazhong University of Science and Technology. Her research interests lie within deep learning for Computer Vision, particularly in large vision models, model generalization and data-centric problems.\nActively looking for internships!","tags":null,"title":"Yang Yang","type":"authors"},{"authors":["Yang Yang","Wenhai Wang","Zhe Chen","Jifeng Dai","Liang Zheng"],"categories":[],"content":"","date":1701484244,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1701484244,"objectID":"4b6d4925f551e92477c8c1782d01677d","permalink":"https://yangyanggirl.github.io/publication/bos/","publishdate":"2023-12-02T10:30:44+08:00","relpermalink":"/publication/bos/","section":"publication","summary":"Bounding boxes uniquely characterize object detection, where a good detector gives accurate bounding boxes of categories of interest. However, in the real-world where test ground truths are not provided, it is non-trivial to find out whether bounding boxes are accurate, thus preventing us from assessing the detector generalization ability. In this work, we find under feature map dropout, good detectors tend to output bounding boxes whose locations do not change much, while bounding boxes of poor detectors will undergo noticeable position changes. We compute the box stability score (BS score) to reflect this stability. Specifically, given an image, we compute a normal set of bounding boxes and a second set after feature map dropout. To obtain BS score, we use bipartite matching to find the corresponding boxes between the two sets and compute the average Intersection over Union (IoU) across the entire test set. We contribute to finding that BS score has a strong, positive correlation with detection accuracy measured by mean average precision (mAP) under various test environments. This relationship allows us to predict the accuracy of detectors on various real-world test sets without accessing test ground truths, verified on canonical detection tasks such as vehicle detection and pedestrian detection.","tags":[],"title":"Bounding Box Stability against Feature Dropout Reflects Detector Generalization across Environments [iclr2024 spotlight]","type":"publication"},{"authors":["Xiaoxiao Sun","Xingjian Leng","Zijian Wang","Yang Yang","Zi Huang","Liang Zheng"],"categories":[],"content":"","date":1696818644,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696818644,"objectID":"2b10536c0d70583fb096a23f9e07a5f0","permalink":"https://yangyanggirl.github.io/publication/cifar10w/","publishdate":"2023-10-09T10:30:44+08:00","relpermalink":"/publication/cifar10w/","section":"publication","summary":"Analyzing model performance in various unseen environments is a critical research problem in the machine learning community. To study this problem, it is important to construct a testbed with out-of-distribution test sets that have broad coverage of environmental discrepancies. However, existing testbeds typically either have a small number of domains or are synthesized by image corruptions, hindering algorithm design that demonstrates real-world effectiveness. In this paper, we introduce CIFAR-10-Warehouse, consisting of 180 datasets collected by prompting image search engines and diffusion models in various ways. Generally sized between 300 and 8,000 images, the datasets contain natural images, cartoons, certain colors, or objects that do not naturally appear. With CIFAR-10-W, we aim to enhance the evaluation and deepen the understanding of two generalization tasks: domain generalization and model accuracy prediction in various out-of-distribution environments. We conduct extensive benchmarking and comparison experiments and show that CIFAR-10-W offers new and interesting insights inherent to these tasks. We also discuss other fields that would benefit from CIFAR-10-W. Data and code are available at https://sites.google.com/view/cifar-10-warehouse/.","tags":[],"title":"CIFAR-10-Warehouse: Broad and More Realistic Testbeds in Model Generalization Analysis [iclr2024]","type":"publication"},{"authors":["Zhaoyang Liu","Yinan He","Wenhai Wang","Weiyun Wang","Yi Wang","Shoufa Chen","Qinglong Zhang","Yang Yang","Qingyun Li","Jiashuo Yu","Kunchang Li","Zhe Chen","Xue Yang","Xizhou Zhu","Yali Wang","Limin Wang","Ping Luo","Jifeng Dai","Yu Qiao"],"categories":[],"content":"","date":1696764644,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696764644,"objectID":"3f5fbce257a5046e46ad198dd857eea9","permalink":"https://yangyanggirl.github.io/publication/interngpt/","publishdate":"2023-05-09T19:30:44+08:00","relpermalink":"/publication/interngpt/","section":"publication","summary":"We present an interactive visual framework named InternGPT, or iGPT for short. The framework integrates chatbots that have planning and reasoning capabilities, such as ChatGPT, with non-verbal instructions like pointing movements that enable users to directly manipulate images or videos on the screen. Pointing (including gestures, cursors, etc.) movements can provide more flexibility and precision in performing vision-centric tasks that require fine-grained control, editing, and generation of visual content. The name InternGPT stands for \textbf{inter}action, \textbf{n}onverbal, and \textbf{chat}bots. Different from existing interactive systems that rely on pure language, by incorporating pointing instructions, the proposed iGPT significantly improves the efficiency of communication between users and chatbots, as well as the accuracy of chatbots in vision-centric tasks, especially in complicated visual scenarios where the number of objects is greater than 2. Additionally, in iGPT, an auxiliary control mechanism is used to improve the control capability of LLM, and a large vision-language model termed Husky is fine-tuned for high-quality multi-modal dialogue (impressing ChatGPT-3.5-turbo with 93.89 percent GPT-4 Quality). We hope this work can spark new ideas and directions for future interactive visual systems. Welcome to watch the code at https://github.com/OpenGVLab/InternGPT.","tags":[],"title":"Internchat: Solving vision-centric tasks by interacting with chatbots beyond language","type":"publication"},{"authors":["Yang Yang","Akshay Asthana","Liang Zheng"],"categories":[],"content":"","date":1646220644,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646220644,"objectID":"891f49f7245949d22ae7e98301979d23","permalink":"https://yangyanggirl.github.io/publication/fg2021/","publishdate":"2022-03-02T19:30:44+08:00","relpermalink":"/publication/fg2021/","section":"publication","summary":"Abstract — This paper studies the benefit of keypoint estimation on object detection. In particular, we focus on the paradigmatic one-stage and two-stage methods, two main categories in the object detection community. We note that while there has been remarkable progress on object detection and keypoint detection, insights on how the latter would beneﬁt the former are somehow lacking. In this paper, we make two contributions. As a major contribution, we point out that one-stage and two-stage detectors have different abilities in accommodating keypoint description. The difference is clearly shown in our experiment where multiple detectors are compared in various detection tasks. Our essential observation is that one-stage detectors beneﬁt consistently from the inclusion of a keypoint detection branch, while for two-stage detectors such beneﬁt is obscure. As a minor contribution, we make several variant designs to improve the trade-off between efﬁciency and accuracy of the one-stage CenterNet on multiple detection tasks.","tags":[],"title":"Does Keypoint Estimation Beneﬁt Object Detection? An Empirical Study of One-stage and Two-stage Detectors [fg2021]","type":"publication"}]