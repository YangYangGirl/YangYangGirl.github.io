[{"authors":["admin"],"categories":null,"content":"Yang Yang is a second-year Computer Science Ph.D. student at Australian National University, supervised by Prof. Liang Zheng. She obtained her B.E. degree at the School of Electronic Information and Communications, Huazhong University of Science and Technology. Her research interests lie within deep learning for Computer Vision, particularly in Dataset-centered Vision and Object Detection.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://yangyanggirl.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Yang Yang is a second-year Computer Science Ph.D. student at Australian National University, supervised by Prof. Liang Zheng. She obtained her B.E. degree at the School of Electronic Information and Communications, Huazhong University of Science and Technology. Her research interests lie within deep learning for Computer Vision, particularly in Dataset-centered Vision and Object Detection.","tags":null,"title":"Yang Yang","type":"authors"},{"authors":["Zhaoyang Liu","Yinan He","Wenhai Wang","Weiyun Wang","Yi Wang","Shoufa Chen","Qinglong Zhang","Yang Yang","Qingyun Li","Jiashuo Yu","Kunchang Li","Zhe Chen","Xue Yang","Xizhou Zhu","Yali Wang","Limin Wang","Ping Luo","Jifeng Dai","Yu Qiao"],"categories":[],"content":"","date":1696764644,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696764644,"objectID":"3f5fbce257a5046e46ad198dd857eea9","permalink":"https://yangyanggirl.github.io/publication/interngpt/","publishdate":"2023-05-09T19:30:44+08:00","relpermalink":"/publication/interngpt/","section":"publication","summary":"We present an interactive visual framework named InternGPT, or iGPT for short. The framework integrates chatbots that have planning and reasoning capabilities, such as ChatGPT, with non-verbal instructions like pointing movements that enable users to directly manipulate images or videos on the screen. Pointing (including gestures, cursors, etc.) movements can provide more flexibility and precision in performing vision-centric tasks that require fine-grained control, editing, and generation of visual content. The name InternGPT stands for \textbf{inter}action, \textbf{n}onverbal, and \textbf{chat}bots. Different from existing interactive systems that rely on pure language, by incorporating pointing instructions, the proposed iGPT significantly improves the efficiency of communication between users and chatbots, as well as the accuracy of chatbots in vision-centric tasks, especially in complicated visual scenarios where the number of objects is greater than 2. Additionally, in iGPT, an auxiliary control mechanism is used to improve the control capability of LLM, and a large vision-language model termed Husky is fine-tuned for high-quality multi-modal dialogue (impressing ChatGPT-3.5-turbo with 93.89 percent GPT-4 Quality). We hope this work can spark new ideas and directions for future interactive visual systems. Welcome to watch the code at https://github.com/OpenGVLab/InternGPT.","tags":[],"title":"Internchat: Solving vision-centric tasks by interacting with chatbots beyond language","type":"publication"},{"authors":["Yang Yang","Akshay Asthana","Liang Zheng"],"categories":[],"content":"","date":1646220644,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646220644,"objectID":"891f49f7245949d22ae7e98301979d23","permalink":"https://yangyanggirl.github.io/publication/fg2021/","publishdate":"2022-03-02T19:30:44+08:00","relpermalink":"/publication/fg2021/","section":"publication","summary":"Abstract — This paper studies the benefit of keypoint estimation on object detection. In particular, we focus on the paradigmatic one-stage and two-stage methods, two main categories in the object detection community. We note that while there has been remarkable progress on object detection and keypoint detection, insights on how the latter would beneﬁt the former are somehow lacking. In this paper, we make two contributions. As a major contribution, we point out that one-stage and two-stage detectors have different abilities in accommodating keypoint description. The difference is clearly shown in our experiment where multiple detectors are compared in various detection tasks. Our essential observation is that one-stage detectors beneﬁt consistently from the inclusion of a keypoint detection branch, while for two-stage detectors such beneﬁt is obscure. As a minor contribution, we make several variant designs to improve the trade-off between efﬁciency and accuracy of the one-stage CenterNet on multiple detection tasks.","tags":[],"title":"Does Keypoint Estimation Beneﬁt Object Detection? An Empirical Study of One-stage and Two-stage Detectors","type":"publication"}]